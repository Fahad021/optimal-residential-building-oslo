{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXFBYdAz7dAU"
   },
   "source": [
    "Code used to find the optimal number of past values and number of neurons for the final neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XKC3zsyQJXlN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.layers import concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "import keras\n",
    "from keras import backend as K "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "TV02bRW7Jbow"
   },
   "outputs": [],
   "source": [
    "\"\"\"Learning to supervised model\"\"\"\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HDQYxjtrJp3x"
   },
   "outputs": [],
   "source": [
    "room = pd.read_csv('/home/ict4bd/Residential_oslo/simulation_data/bath.csv', parse_dates=[0,], index_col=\"Date_Time\")\n",
    "#if you want to study univariate MLP, remove the comment on the following line, the only column remaining will be 't_in'\n",
    "#room = room.drop(['t_out', 'power_h1', 'power_c1', 'solar_rad'], axis=1)\n",
    "number_features = len(room.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NgDk6rOmEGcX",
    "outputId": "0550f715-ee2c-42c3-8c55-df53571d9d81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['t_in'], dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "room.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xMAqcFdoJv9u"
   },
   "outputs": [],
   "source": [
    "values = room.values #change this according to the room in study\n",
    "values = values.astype('float32') # ensure all data is float\n",
    "number_future_samples = 20 #up to five hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vV0B8R8BKn6h",
    "outputId": "c1efab91-3dcb-4f81-9741-6fc57a74d9ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0717 - val_loss: 0.0496\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0395 - val_loss: 0.0494\n",
      "Epoch 3/5\n",
      "1460/1460 - 2s - loss: 0.0377 - val_loss: 0.0500\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0360 - val_loss: 0.0464\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0348 - val_loss: 0.0434\n",
      "1 past value - MAE 0.5308\n",
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0574 - val_loss: 0.0595\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0413 - val_loss: 0.0629\n",
      "Epoch 3/5\n",
      "1460/1460 - 1s - loss: 0.0390 - val_loss: 0.0566\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0371 - val_loss: 0.0544\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0359 - val_loss: 0.0498\n",
      "2 past value - MAE 0.6092\n",
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0572 - val_loss: 0.0690\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0429 - val_loss: 0.0652\n",
      "Epoch 3/5\n",
      "1460/1460 - 2s - loss: 0.0396 - val_loss: 0.0564\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0372 - val_loss: 0.0570\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0362 - val_loss: 0.0532\n",
      "3 past value - MAE 0.6500\n",
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0563 - val_loss: 0.0710\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0441 - val_loss: 0.0694\n",
      "Epoch 3/5\n",
      "1460/1460 - 2s - loss: 0.0397 - val_loss: 0.0641\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0370 - val_loss: 0.0630\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0362 - val_loss: 0.0555\n",
      "4 past value - MAE 0.6777\n",
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0565 - val_loss: 0.0745\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0445 - val_loss: 0.0729\n",
      "Epoch 3/5\n",
      "1460/1460 - 2s - loss: 0.0399 - val_loss: 0.0619\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0366 - val_loss: 0.0627\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0346 - val_loss: 0.0547\n",
      "5 past value - MAE 0.6688\n",
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0572 - val_loss: 0.0811\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0451 - val_loss: 0.0686\n",
      "Epoch 3/5\n",
      "1460/1460 - 2s - loss: 0.0396 - val_loss: 0.0631\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0359 - val_loss: 0.0629\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0351 - val_loss: 0.0578\n",
      "6 past value - MAE 0.7059\n",
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0564 - val_loss: 0.0796\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0445 - val_loss: 0.0710\n",
      "Epoch 3/5\n",
      "1460/1460 - 2s - loss: 0.0386 - val_loss: 0.0705\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0358 - val_loss: 0.0615\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0341 - val_loss: 0.0594\n",
      "7 past value - MAE 0.7263\n",
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0579 - val_loss: 0.0892\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0438 - val_loss: 0.0808\n",
      "Epoch 3/5\n",
      "1460/1460 - 1s - loss: 0.0391 - val_loss: 0.0713\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0371 - val_loss: 0.0651\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0351 - val_loss: 0.0568\n",
      "8 past value - MAE 0.6943\n",
      "Epoch 1/5\n",
      "1460/1460 - 2s - loss: 0.0585 - val_loss: 0.0889\n",
      "Epoch 2/5\n",
      "1460/1460 - 2s - loss: 0.0456 - val_loss: 0.0805\n",
      "Epoch 3/5\n",
      "1460/1460 - 2s - loss: 0.0403 - val_loss: 0.0755\n",
      "Epoch 4/5\n",
      "1460/1460 - 2s - loss: 0.0367 - val_loss: 0.0622\n",
      "Epoch 5/5\n",
      "1460/1460 - 2s - loss: 0.0352 - val_loss: 0.0608\n",
      "9 past value - MAE 0.7427\n"
     ]
    }
   ],
   "source": [
    "#OPTIMIZATION LAG\n",
    "#find the best lag order considering from 1 to 15 past values\n",
    "\n",
    "MAE_Past_Values_List = [] #store the value of average prediction error for each lag order\n",
    "\n",
    "for number_past_values in range (1,10):\n",
    "\n",
    "  #Preparation of the dataset\n",
    "  reframed = series_to_supervised(values, number_past_values, number_future_samples)\n",
    "  input_past_values = reframed.iloc[:,0:number_past_values * number_features]\n",
    "  output_future_values = reframed.iloc[:,number_past_values * number_features:]\n",
    "  output_future_values = reframed.filter(regex=r'(var1\\(t\\+|var1\\(t\\))')\n",
    "  prepared_dataset = pd.concat([input_past_values, output_future_values], axis = 1 )\n",
    "\n",
    "  #Splitting dataset into training tand test set\n",
    "  to_be_scaled_values = prepared_dataset.values\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "  scaled_values = scaler.fit_transform(to_be_scaled_values)\n",
    "  n_train_hours = 3 * 365 * 24 * 4  #number of quaters of hour in 3 years time \n",
    "\n",
    "  train = scaled_values[:n_train_hours, :]\n",
    "  test = scaled_values[n_train_hours:, :]\n",
    "  train_X, train_y = train[:, :-(number_future_samples)], train[:, -(number_future_samples):]\n",
    "  test_X, test_y = test[:, :-(number_future_samples)], test[:, -(number_future_samples):]\n",
    "\n",
    "  #Definition of the mode\n",
    "  model = Sequential()\n",
    "  model.add(Dense(30, activation='sigmoid', input_dim = train_X.shape[1],kernel_initializer='random_normal')) \n",
    "  model.add(Dense(number_future_samples))\n",
    "  optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "  model.compile(optimizer=optimizer, loss='MAE')\n",
    "\n",
    "  #Model Fitting doing the average of three fitting\n",
    "  yhat_list = np.zeros((test_X.shape[0],number_future_samples))\n",
    "  j = 0\n",
    "  for i in range (1,4):\n",
    "    history = model.fit(train_X, train_y, epochs = 100, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "    #Prediction\n",
    "    yhat = model.predict(test_X)\n",
    "    yhat_list = yhat_list + yhat\n",
    "    j = j + 1\n",
    "  yhat = yhat_list/j\n",
    "  \n",
    "  #Invert scaling of the test vector \n",
    "  inv_test_y = concatenate((test_X,test_y),axis = 1)\n",
    "  inv_test_y = scaler.inverse_transform(inv_test_y)\n",
    "  inv_test_y = inv_test_y[:,-number_future_samples:]\n",
    "\n",
    " #Invert scaling predicted y_hat\n",
    "  inv_yhat = concatenate((test_X,yhat),axis = 1)\n",
    "  inv_yhat = np.array(inv_yhat)\n",
    "  inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "  inv_yhat = inv_yhat[:,-number_future_samples:]\n",
    "\n",
    "  MAE_list = [] #vector storing the MAE of the future prediction (from the next step to 20 step ahead)\n",
    "  for i in range (inv_yhat.shape[1]):\n",
    "    MAE = mean_absolute_error(inv_test_y[:,i],inv_yhat[:,i])\n",
    "    MAE_list.append(MAE)\n",
    "\n",
    "  MAE_Past_Values_List.append(MAE_list)\n",
    "  MAE_list = np.array(MAE_list)\n",
    "\n",
    "  print('%d past value - MAE %.4f' %(number_past_values,MAE_list.mean())) #Append the average prediction error \n",
    "\n",
    "  del model\n",
    "  K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaGgKsvh937V",
    "outputId": "c759431e-1d4a-4a36-ff75-20c96eea2bdb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE_Past_Values_List = np.array(MAE_Past_Values_List)\n",
    "\n",
    "optimal_lag_value = np.argmin(MAE_Past_Values_List) + 1\n",
    "optimal_lag_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kan3QrRtp4r",
    "outputId": "9945b106-6563-40a1-8ee9-b23ad248b995"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training past value 1 with 1 neurons over.\n",
      "Training past value 1 with 2 neurons over.\n",
      "Training past value 1 with 3 neurons over.\n",
      "Training past value 1 with 4 neurons over.\n",
      "Training past value 1 with 5 neurons over.\n",
      "Training past value 1 with 6 neurons over.\n",
      "Training past value 1 with 7 neurons over.\n",
      "Training past value 1 with 8 neurons over.\n",
      "Training past value 1 with 9 neurons over.\n",
      "Training past value 1 with 10 neurons over.\n",
      "Training past value 1 with 11 neurons over.\n",
      "Training past value 1 with 12 neurons over.\n",
      "Training past value 1 with 13 neurons over.\n",
      "Training past value 1 with 14 neurons over.\n",
      "Optimal number of neurons for past value 1 = 6\n"
     ]
    }
   ],
   "source": [
    "#OPTIMIZATION NUMBER NEURONS\n",
    "\n",
    "neuron_number_list = np.arange(1,15,1)\n",
    "\n",
    "for number_past_values in [optimal_lag_value]:\n",
    "\n",
    "  #Preparation of the dataset\n",
    "  reframed = series_to_supervised(values, number_past_values, number_future_samples)\n",
    "  input_past_values = reframed.iloc[:,0:number_past_values * number_features]\n",
    "  output_future_values = reframed.iloc[:,number_past_values * number_features:]\n",
    "  output_future_values = reframed.filter(regex=r'(var1\\(t\\+|var1\\(t\\))')\n",
    "  prepared_dataset = pd.concat([input_past_values, output_future_values], axis = 1 )\n",
    "\n",
    "  #Splitting dataset into training tand test set\n",
    "  to_be_scaled_values = prepared_dataset.values\n",
    "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "  scaled_values = scaler.fit_transform(to_be_scaled_values)\n",
    "  n_train_hours = 3 * 365 * 24 * 4  #number of quaters of hour in 3 years time \n",
    "\n",
    "  train = scaled_values[:n_train_hours, :]\n",
    "  test = scaled_values[n_train_hours:, :]\n",
    "  train_X, train_y = train[:, :-(number_future_samples)], train[:, -(number_future_samples):]\n",
    "  test_X, test_y = test[:, :-(number_future_samples)], test[:, -(number_future_samples):]\n",
    "\n",
    "  MAE_neuron_number = []\n",
    "\n",
    "  #Grid Search Number Neurons:\n",
    "  for neuron_number in neuron_number_list:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neuron_number, activation='sigmoid', input_dim = train_X.shape[1])) \n",
    "    model.add(Dense(number_future_samples))\n",
    "    optimizer = keras.optimizers.Adam(lr=0.01)\n",
    "    model.compile(optimizer=optimizer, loss='mae')\n",
    "\n",
    "    #Model Fitting\n",
    "    history = model.fit(train_X, train_y, epochs = 200, batch_size=72, validation_data=(test_X, test_y), verbose=0, shuffle=False)\n",
    "    #Prediction\n",
    "    yhat = model.predict(test_X)\n",
    "\n",
    "    #Invert scaling of the test vector \n",
    "    inv_test_y = concatenate((test_X,test_y),axis = 1)\n",
    "    inv_test_y = scaler.inverse_transform(inv_test_y)\n",
    "    inv_test_y = inv_test_y[:,-number_future_samples:]\n",
    "\n",
    "  #Invert scaling predicted y_hat\n",
    "    inv_yhat = concatenate((test_X,yhat),axis = 1)\n",
    "    inv_yhat = np.array(inv_yhat)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,-number_future_samples:]\n",
    "\n",
    "    MAE_future_step_neuron = [] #store the MAE at everychange of number of neurons\n",
    "    for i in range (inv_yhat.shape[1]):\n",
    "      MAE = mean_absolute_error(inv_test_y[:,i],inv_yhat[:,i])\n",
    "      MAE_future_step_neuron.append(MAE)\n",
    "    MAE_neuron_number.append(MAE_future_step_neuron)\n",
    "\n",
    "    print('Training past value %d with %d neurons over.' %(number_past_values, neuron_number))\n",
    "  \n",
    "  MAE_neuron_number = np.array(MAE_neuron_number)\n",
    "  MAE_variation_neurons = []\n",
    "  for i in range (MAE_neuron_number.shape[0]):\n",
    "    MAE_variation_neurons.append(MAE_neuron_number[i,:].mean())\n",
    "  optimal_neuron_number_index = np.argmin(MAE_variation_neurons)\n",
    "  optimal_neuron_number = neuron_number_list[optimal_neuron_number_index]\n",
    "  \n",
    "  print ('Optimal number of neurons for past value %d = %d' %(number_past_values, optimal_neuron_number))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ModelTuning.py",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
